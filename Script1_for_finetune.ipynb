{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvXt0X3aKBU3WZ84bZ4zMY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saadkhi/Side/blob/main/Script1_for_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hVzapt_Rxp4",
        "outputId": "641f0df7-cfd1-4d82-e8ba-e4caeaeb60da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "# import pandas as pd\n",
        "# from collections import Counter\n",
        "\n",
        "# # Step 1: Read JSONL file\n",
        "# def read_jsonl(file_path):\n",
        "#     data = []\n",
        "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
        "#         for line in f:\n",
        "#             try:\n",
        "#                 data.append(json.loads(line.strip()))\n",
        "#             except json.JSONDecodeError as e:\n",
        "#                 print(f\"Error parsing line: {line.strip()} - {e}\")\n",
        "#     return data\n",
        "\n",
        "# # Step 2: Verify dataset\n",
        "# def check_dataset(data):\n",
        "#     print(f\"Total rows: {len(data)}\")\n",
        "\n",
        "#     # Check for missing keys\n",
        "#     for i, entry in enumerate(data, 1):\n",
        "#         if \"prompt\" not in entry or \"completion\" not in entry:\n",
        "#             print(f\"Missing keys in row {i}: {entry}\")\n",
        "\n",
        "#     # Check balance of positive vs. negative examples\n",
        "#     completions = [entry[\"completion\"] for entry in data]\n",
        "#     rejection_count = sum(1 for c in completions if \"Sorry, I can only answer\" in c)\n",
        "#     print(f\"Positive (SQL/NoSQL) examples: {len(data) - rejection_count}\")\n",
        "#     print(f\"Negative (unrelated) examples: {rejection_count}\")\n",
        "\n",
        "#     # Check for duplicates\n",
        "#     prompts = [entry[\"prompt\"] for entry in data]\n",
        "#     duplicates = [prompt for prompt, count in Counter(prompts).items() if count > 1]\n",
        "#     if duplicates:\n",
        "#         print(f\"Duplicate prompts found: {duplicates}\")\n",
        "#     else:\n",
        "#         print(\"No duplicate prompts found.\")\n",
        "\n",
        "# # Step 3: Create DataFrame\n",
        "# def create_dataframe(data):\n",
        "#     # Convert to DataFrame\n",
        "#     df = pd.DataFrame(data)\n",
        "\n",
        "#     # Add is_related column: True if completion doesn't contain rejection message\n",
        "#     df['is_related'] = ~df['completion'].str.contains(\"Sorry, I can only answer\")\n",
        "\n",
        "#     return df\n",
        "\n",
        "# # Step 4: Analyze DataFrame\n",
        "# def analyze_dataframe(df):\n",
        "#     print(\"\\nDataFrame Info:\")\n",
        "#     df.info()\n",
        "\n",
        "#     print(\"\\nFirst 5 rows:\")\n",
        "#     df.head()\n",
        "\n",
        "#     print(\"\\nDistribution of is_related:\")\n",
        "#     df['is_related'].value_counts()\n",
        "\n",
        "#     # Check for missing values\n",
        "#     print(\"\\nMissing values:\")\n",
        "#     df.isnull().sum()\n",
        "\n",
        "# # Main execution\n",
        "# file_path = \"dataset.jsonl\"  # Update with your file path\n",
        "# data = read_jsonl(file_path)\n",
        "# check_dataset(data)\n",
        "# df = create_dataframe(data)\n",
        "# analyze_dataframe(df)\n",
        "\n",
        "# # Optional: Save DataFrame to CSV for further use\n",
        "# df.to_csv(\"database_dataset.csv\", index=False)\n",
        "# print(\"\\nDataFrame saved as 'database_dataset.csv'\")"
      ],
      "metadata": {
        "id": "fzCTboyBxLsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Read file line by line to find invalid JSON\n",
        "with open('dataset.jsonl', 'r') as f:\n",
        "    for i, line in enumerate(f, 1):\n",
        "        try:\n",
        "            json.loads(line.strip())  # Try parsing each line\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Error in line {i}: {line.strip()} - {e}\")"
      ],
      "metadata": {
        "id": "09jUveWx9qvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the corrected JSONL file\n",
        "try:\n",
        "    df = pd.read_json('dataset.jsonl', lines=True)\n",
        "    print(\"DataFrame loaded successfully!\")\n",
        "except ValueError as e:\n",
        "    print(f\"Still an error: {e}. Please check the JSONL file for formatting issues.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDwHY04n9rFe",
        "outputId": "e66c8641-4b55-44e7-db3a-4551b066b5b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "X2W_SU88-ToX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "f6e5498f-0e60-4809-c8ba-5d614857ea94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              prompt  \\\n",
              "0  Write a SQL query to find all employees with s...   \n",
              "1  How do I join two tables in SQL to get custome...   \n",
              "2  Explain the difference between INNER JOIN and ...   \n",
              "3  Write a SQL query to get the top 5 products by...   \n",
              "4  How to create a table in MySQL for storing use...   \n",
              "\n",
              "                                          completion  \n",
              "0      SELECT * FROM employees WHERE salary > 50000;  \n",
              "1  SELECT c.customer_id, c.name, o.order_id FROM ...  \n",
              "2  INNER JOIN returns only matching rows from bot...  \n",
              "3  SELECT product_name, SUM(sales) as total_sales...  \n",
              "4  CREATE TABLE users (id INT AUTO_INCREMENT PRIM...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fd74f370-6227-49f4-b3c0-740785e56f65\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "      <th>completion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Write a SQL query to find all employees with s...</td>\n",
              "      <td>SELECT * FROM employees WHERE salary &gt; 50000;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How do I join two tables in SQL to get custome...</td>\n",
              "      <td>SELECT c.customer_id, c.name, o.order_id FROM ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Explain the difference between INNER JOIN and ...</td>\n",
              "      <td>INNER JOIN returns only matching rows from bot...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Write a SQL query to get the top 5 products by...</td>\n",
              "      <td>SELECT product_name, SUM(sales) as total_sales...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How to create a table in MySQL for storing use...</td>\n",
              "      <td>CREATE TABLE users (id INT AUTO_INCREMENT PRIM...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fd74f370-6227-49f4-b3c0-740785e56f65')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fd74f370-6227-49f4-b3c0-740785e56f65 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fd74f370-6227-49f4-b3c0-740785e56f65');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-6a5ed0ef-6afc-4a86-8a2f-8152246c21c9\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6a5ed0ef-6afc-4a86-8a2f-8152246c21c9')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-6a5ed0ef-6afc-4a86-8a2f-8152246c21c9 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 6030,\n  \"fields\": [\n    {\n      \"column\": \"prompt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3545,\n        \"samples\": [\n          \"SQL query to use CURRENT_TIMESTAMP\",\n          \"Explain cursors in SQL.\",\n          \"Write a Neo4j query to find nodes by relationship count.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"completion\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3254,\n        \"samples\": [\n          \"Shard key determines data distribution across shards for scalability.\",\n          \"Constraints like PRIMARY KEY, FOREIGN KEY, NOT NULL enforce data integrity.\",\n          \"SELECT * FROM table_name WHERE column BETWEEN 10 AND 20;\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IXfQylE-yM_",
        "outputId": "d2c192e1-add6-4d4e-c606-fb1cbf2e9cb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6030, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing or empty values\n",
        "missing_prompts = df[df['prompt'].isnull() | (df['prompt'] == '')]\n",
        "missing_completions = df[df['completion'].isnull() | (df['completion'] == '')]\n",
        "print(\"Missing/empty prompts:\", len(missing_prompts))\n",
        "print(\"Missing/empty completions:\", len(missing_completions))\n",
        "print(\"Missing prompts sample:\\n\", missing_prompts[['prompt', 'completion']].head())\n",
        "print(\"Missing completions sample:\\n\", missing_completions[['prompt', 'completion']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLWQy9mW-2Ui",
        "outputId": "184ab4ca-84d2-4a6d-fb10-f3c176ad1824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing/empty prompts: 0\n",
            "Missing/empty completions: 0\n",
            "Missing prompts sample:\n",
            " Empty DataFrame\n",
            "Columns: [prompt, completion]\n",
            "Index: []\n",
            "Missing completions sample:\n",
            " Empty DataFrame\n",
            "Columns: [prompt, completion]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicate prompts\n",
        "duplicates = df[df.duplicated(subset=['prompt'], keep=False)]\n",
        "print(\"Duplicate prompts count:\", len(duplicates))\n",
        "print(\"Duplicate prompts sample:\\n\", duplicates[['prompt', 'completion']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgjUumHY_FJS",
        "outputId": "1df35ba5-312f-4053-9bcb-c17e3db18788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplicate prompts count: 3475\n",
            "Duplicate prompts sample:\n",
            "                                                prompt  \\\n",
            "0   Write a SQL query to find all employees with s...   \n",
            "2   Explain the difference between INNER JOIN and ...   \n",
            "12    Write a SQL query to find second-highest salary   \n",
            "13                      How to use a subquery in SQL?   \n",
            "20                    Write a SQL query to pivot data   \n",
            "\n",
            "                                           completion  \n",
            "0       SELECT * FROM employees WHERE salary > 50000;  \n",
            "2   INNER JOIN returns only matching rows from bot...  \n",
            "12  SELECT MAX(salary) FROM employees WHERE salary...  \n",
            "13  A subquery is a query nested inside another, e...  \n",
            "20  In PostgreSQL, use CROSSTAB or CASE: SELECT de...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check prompt and completion lengths for outliers\n",
        "df['prompt_length'] = df['prompt'].str.len()\n",
        "df['completion_length'] = df['completion'].str.len()\n",
        "short_prompts = df[df['prompt_length'] < 10]\n",
        "long_prompts = df[df['prompt_length'] > 2000]\n",
        "short_completions = df[df['completion_length'] < 10]\n",
        "long_completions = df[df['completion_length'] > 2000]\n",
        "print(\"Short prompts (<10 chars):\", len(short_prompts))\n",
        "print(\"Long prompts (>2000 chars):\", len(long_prompts))\n",
        "print(\"Short completions (<10 chars):\", len(short_completions))\n",
        "print(\"Long completions (>2000 chars):\", len(long_completions))\n",
        "print(\"Short prompts sample:\\n\", short_prompts[['prompt', 'completion']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVn9wNuw_JG6",
        "outputId": "82f70ba5-2443-4d0d-dd6b-2507e4f786fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Short prompts (<10 chars): 0\n",
            "Long prompts (>2000 chars): 0\n",
            "Short completions (<10 chars): 31\n",
            "Long completions (>2000 chars): 0\n",
            "Short prompts sample:\n",
            " Empty DataFrame\n",
            "Columns: [prompt, completion]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for inconsistent refusal messages\n",
        "refusal_pattern = \"Sorry, I can only answer questions related to SQL, NoSQL, and the database domain.\"\n",
        "inconsistent_refusals = df[(df['completion'].str.startswith(\"Sorry\", na=False) & (df['completion'] != refusal_pattern))]\n",
        "print(\"Inconsistent refusals count:\", len(inconsistent_refusals))\n",
        "print(\"Inconsistent refusals sample:\\n\", inconsistent_refusals[['prompt', 'completion']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXZt7moI_N9V",
        "outputId": "b37d6b7d-025b-4c66-e0f1-0b2c4228fe56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inconsistent refusals count: 1072\n",
            "Inconsistent refusals sample:\n",
            "                               prompt  \\\n",
            "68         What’s the weather today?   \n",
            "69  How to code a website in Python?   \n",
            "70     What’s the capital of France?   \n",
            "71   Explain machine learning basics   \n",
            "72         Query the meaning of life   \n",
            "\n",
            "                                           completion  \n",
            "68  Sorry, I can only answer questions related to ...  \n",
            "69  Sorry, I can only answer questions related to ...  \n",
            "70  Sorry, I can only answer questions related to ...  \n",
            "71  Sorry, I can only answer questions related to ...  \n",
            "72  Sorry, I can only answer questions related to ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic check for potentially invalid SQL/NoSQL completions (approximate)\n",
        "positive_examples = df[~df['completion'].str.startswith(\"Sorry\", na=False)]\n",
        "missing_sql_keywords = positive_examples[\n",
        "    positive_examples['completion'].str.contains('SELECT|INSERT|UPDATE|DELETE|CREATE|ALTER|DROP', case=False, na=False) == False\n",
        "]\n",
        "missing_nosql_keywords = positive_examples[\n",
        "    positive_examples['completion'].str.contains('db.collection|find|aggregate|insert|update|delete|KEYS|HSET|SMEMBERS|UPDATE keyspace', case=False, na=False) == False\n",
        "]\n",
        "print(\"Potential invalid SQL completions (no common keywords):\", len(missing_sql_keywords))\n",
        "print(\"Potential invalid NoSQL completions (no common keywords):\", len(missing_nosql_keywords))\n",
        "print(\"Potential invalid SQL sample:\\n\", missing_sql_keywords[['prompt', 'completion']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tlbBQaQ_Qz8",
        "outputId": "a1f3f4e3-2546-4840-ccc0-3c90ffaa286c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Potential invalid SQL completions (no common keywords): 2836\n",
            "Potential invalid NoSQL completions (no common keywords): 3590\n",
            "Potential invalid SQL sample:\n",
            "                                                prompt  \\\n",
            "2   Explain the difference between INNER JOIN and ...   \n",
            "6                        What’s a primary key in SQL?   \n",
            "8                   How to handle NULL values in SQL?   \n",
            "10  What’s the difference between GROUP BY and ORD...   \n",
            "19  What’s the difference between SQL and NoSQL da...   \n",
            "\n",
            "                                           completion  \n",
            "2   INNER JOIN returns only matching rows from bot...  \n",
            "6   A primary key is a unique, non-null column (or...  \n",
            "8   Use IS NULL or IS NOT NULL to check for NULLs,...  \n",
            "10  GROUP BY aggregates rows into groups based on ...  \n",
            "19  SQL databases are relational, use structured s...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WOM2zeRw_qNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MODEL FINETUNING**"
      ],
      "metadata": {
        "id": "gf6H-dAMuinl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers peft datasets torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ziKkZK9uluw",
        "outputId": "4b53b499-b63c-4301-94e2-66fe8fa6ac5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.4)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft) (1.10.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.8)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_json('dataset.jsonl', lines=True)\n",
        "\n",
        "# Split into train (80%), validation (10%), test (10%)\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "print(\"Train:\", len(train_df), \"Val:\", len(val_df), \"Test:\", len(test_df))\n",
        "\n",
        "# Save splits\n",
        "train_df.to_json('train.jsonl', orient='records', lines=True)\n",
        "val_df.to_json('val.jsonl', orient='records', lines=True)\n",
        "test_df.to_json('test.jsonl', orient='records', lines=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-1KMSXTum6c",
        "outputId": "248475bd-851f-4d35-9ec4-880b597d186e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 4824 Val: 603 Test: 603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load tokenizer (replace with your model, e.g., \"distilgpt2\" or \"meta-llama/Llama-2-7b-hf\")\n",
        "model_name = \"distilgpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Add padding token if needed\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "\n",
        "# Tokenize function\n",
        "def tokenize_function(examples):\n",
        "    inputs = tokenizer(examples['prompt'], padding=\"max_length\", truncation=True, max_length=512)\n",
        "    outputs = tokenizer(examples['completion'], padding=\"max_length\", truncation=True, max_length=512)\n",
        "    inputs['labels'] = outputs['input_ids']\n",
        "    return inputs\n",
        "\n",
        "# Apply tokenization\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "jLqFz_qwvH06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365,
          "referenced_widgets": [
            "e95b22136ce04644bdf18e9abb96aeae",
            "668d4ddc98b04be1913bc1e91a843525",
            "01111ddf878c4c1b9289daae1037019d",
            "dd02bf52ebec4b6383c027fd356fc3ad",
            "42f363746ce2484c9d4cf096f5b57389",
            "d938f9ed57da46c0be18d9f655a26f10",
            "a4559ff75413465ab9204fb39e4f0136",
            "152dfee63f0b41519667c6c1d7e964b7",
            "6c84da0073074dd1932c8236225a7bf3",
            "338c359ac3444c5497cc391677687dde",
            "49e7719dc02643fa88116feb2c8ea1e2",
            "be95975c93c04dd0ac44a9b7c80620ff",
            "a0665525e0444aa28b48d8bba3a3a42b",
            "e0100cb284884382aaca8a34a470f155",
            "3688def848634c0f8d1c9c9d5b969824",
            "ae937a98f0b2416cb689378604795f93",
            "3085daddf96247ada8190c705ceb306f",
            "6f4ec51f24c641508ea95ad7b758860a",
            "448850beca8a4c76bed22b28e676932c",
            "aa74b9cb6eea43a4acc84144cf4f30f8",
            "2ed49b4b2e194d83b765901028eef611",
            "a13ff3985bf34ffc9da49012f289c8e4",
            "cb9082be93084c5eb6e99423cdb8242f",
            "244ca41f1bf842f29e39312ecbb498b4",
            "bec04af8b54048ec8790b49cc444be97",
            "554aa06c5ed9400886c775bee819172c",
            "245f8f990c5c4290974d8323ba408b2e",
            "34d808b7f0ee498a8d8b7c9a4fb1e2ba",
            "f6fcf086bc1347ed86b7ed2df75ea82b",
            "4a45517813f143bc98e1c1bceb882d84",
            "94f30005c4a3424babe4c7dd3fab8236",
            "5171100c06f5474db2390d38a0313c43",
            "21693381ca76466092323be48bb84410",
            "3be8104fad9b4751b5b91d78ed89ebc9",
            "e0e690c65dd84c78b1a5ff6f246a5fa0",
            "8212909d8edf4eacb8bb5b27f48c403a",
            "2add380755964474a975d0fcac55f73d",
            "0fb4c86209154446b5b44b08a41caa79",
            "8608c750daac4df7a3a55f0493678b48",
            "aa067c43404440bc8160562cc92218d4",
            "ffc757a920e14397b18c005b00ae24cf",
            "9681c95714ae4184993064af44c7ba0e",
            "e71fb63a1db64a01aeddf6e232b5d8d3",
            "5392df33ffe548e59a38829488502409",
            "2339906b9e9a4e449929e64ec1dda708",
            "ae4bb7fceb5d43e381d81276598c6398",
            "cff5680d257f499aad554c32bd6c4337",
            "a38c6c05bfd34a0d847b2a00a74b2a97",
            "d1d0ab5b8166466ebed120d5413b8cab",
            "3bce5abbfe574f8c821023ad42fdaa37",
            "49aa4fa6d5f648f0bf67550f6efb4eb8",
            "c0db0be9c9cc4ba1bcaf6c600afc8971",
            "554fb5d2072346928b964a2958825cb0",
            "88aa720473244d4f95cc211ec2d5f744",
            "2d8c3f7d50554395a6359d915deffd8d",
            "17f443d1bafb4eb2bc28c3e08bf9d58e",
            "bfce71c32b6640abab7f8b3d83f8131e",
            "15ab3253d7b34508a0c1aaadf3e2dea8",
            "a66d9163972d424f9294cb63862ff421",
            "d8747f1e601e4ab4ab695b90bc12cbdd",
            "15e1407e1ba9460682b00e869d898fb1",
            "e38f294eb8da4e5da82d338246f5a6e7",
            "d7f002b8ada544c3a9d07dcc8e14d9b1",
            "a945d6ddf1484682a4f0ced256b79ac1",
            "5e54b2f7cb9945ff940ede368dc7a0e1",
            "608bc4b034944e498eb38edd619bac83",
            "4adea73e44724024900c3569232411b1",
            "ea6fec09bfec4d648a0cf183ad0af609",
            "757179d07d474544a62e371326354c0c",
            "8d7f9e7b66024a3bb34d79e046b958e8",
            "acd925941750429b87829c35e6f96803",
            "3e70a098e8af452d9362a9242f491af5",
            "45843fa4468545a681bd60aa0dde1288",
            "cb33141be0964464b2bf5d81faac1c2f",
            "33c1fc4e471c4437b99b1b815a6534b3",
            "3408214404ca448e8bb395a603bbe02c",
            "7953fb7530ea441c95a6dd7a4c72d8fc"
          ]
        },
        "outputId": "b791533a-d889-4a3f-bd47-c744b7886e23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e95b22136ce04644bdf18e9abb96aeae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be95975c93c04dd0ac44a9b7c80620ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cb9082be93084c5eb6e99423cdb8242f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3be8104fad9b4751b5b91d78ed89ebc9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2339906b9e9a4e449929e64ec1dda708"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/4824 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "17f443d1bafb4eb2bc28c3e08bf9d58e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/603 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4adea73e44724024900c3569232411b1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Load model (already done, but included for clarity)\n",
        "model_name = \"distilgpt2\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Configure LoRA with correct target modules for DistilGPT-2\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # Rank of adaptation\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"c_fc\", \"c_proj\"],  # Correct modules for DistilGPT-2's feed-forward layers\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "print(\"LoRA model configured successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "a044f02f921b4503af9403b2bf410b4b",
            "7c9ffb82ff414b42b1d519ac43d2934e",
            "3f57d25bf11b48a9baceeb008662c176",
            "ac8c418d655e4f36ab4aa847ce0df3f9",
            "883a1969b95c407e9edaf6e985139c8b",
            "39d6d385faf94619acaecbbd480ef652",
            "5aa5fb9cb631420f8f3d80ac38fdd782",
            "209673db6aaf4b97a17eb0156ca55105",
            "776b1db1522a45b986e452c663c4feac",
            "1cf9ad88087d49c98b3028b340b1b354",
            "699acaf057e3489b92c936875ce57c1f",
            "5538ecae22d243c989a4c95cf97d2a88",
            "a169f7d15aba4683a52ee7a3513faa26",
            "74c66a68809a4ee8821780b96e3f2466",
            "6a0656fd2e9546b5a06dca2090742f85",
            "50fbbd9953064729bcbe73103acc8fa6",
            "0f130be5a1574ac1988b5be65a0b7043",
            "d2b4d105c1a54258891212a9a82bbfd9",
            "ccf37c3d1b5f4b81a56678e4855c6261",
            "5ce4c89e107d4963b457aa90983e81f1",
            "79cb034cf0734d3e862456d66b4b7325",
            "3e23636e573945c5b2ab84947a0706a4"
          ]
        },
        "id": "j5Rc9yAd4sv7",
        "outputId": "e2affa19-e089-4715-f693-02dd3ddc6a31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a044f02f921b4503af9403b2bf410b4b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5538ecae22d243c989a4c95cf97d2a88"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoRA model configured successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./fine_tuned_model\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    learning_rate=2e-5,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_steps=500,\n",
        "    logging_steps=100,\n",
        "    save_total_limit=2,\n",
        "    fp16=True,  # Enable if GPU supports it\n",
        ")\n",
        "\n",
        "# Initialize trainer (use 'model' from Step 4, not 'model_name')\n",
        "trainer = Trainer(\n",
        "    model=model,  # Corrected: use the model object, not model_name\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset\n",
        ")\n",
        "\n",
        "# Train\n",
        "trainer.train()\n",
        "\n",
        "# Save model\n",
        "model.save_pretrained(\"./fine_tuned_model\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
        "print(\"Training completed and model saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "id": "pQPuY0u4CUwL",
        "outputId": "b29527e6-4e4a-4169-b776-35095812355b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msaadalioffic\u001b[0m (\u001b[33msaadalioffic-usman-institute-of-technology-uit\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250901_114820-kd4q8bt2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/saadalioffic-usman-institute-of-technology-uit/huggingface/runs/kd4q8bt2' target=\"_blank\">whole-cloud-1</a></strong> to <a href='https://wandb.ai/saadalioffic-usman-institute-of-technology-uit/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/saadalioffic-usman-institute-of-technology-uit/huggingface' target=\"_blank\">https://wandb.ai/saadalioffic-usman-institute-of-technology-uit/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/saadalioffic-usman-institute-of-technology-uit/huggingface/runs/kd4q8bt2' target=\"_blank\">https://wandb.ai/saadalioffic-usman-institute-of-technology-uit/huggingface/runs/kd4q8bt2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3618' max='3618' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3618/3618 09:45, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.376300</td>\n",
              "      <td>0.369623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.336100</td>\n",
              "      <td>0.301403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.294000</td>\n",
              "      <td>0.277252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.270200</td>\n",
              "      <td>0.266858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.284500</td>\n",
              "      <td>0.261666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.254000</td>\n",
              "      <td>0.261158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.252700</td>\n",
              "      <td>0.258690</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed and model saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "import torch\n",
        "\n",
        "# Load test dataset\n",
        "test_dataset = Dataset.from_pandas(pd.read_json('test.jsonl', lines=True))\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Evaluate on test set\n",
        "results = trainer.evaluate(test_dataset)\n",
        "print(\"Test set evaluation:\", results)\n",
        "\n",
        "# Manual inspection (check 5 examples)\n",
        "for i in range(5):\n",
        "    input_text = test_df.iloc[i]['prompt']\n",
        "    expected = test_df.iloc[i]['completion']\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    # Move inputs to CUDA (same device as model)\n",
        "    inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
        "    # Ensure model is on CUDA (should already be, but confirm)\n",
        "    model.to('cuda')\n",
        "    outputs = model.generate(**inputs, max_length=512)\n",
        "    predicted = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"Prompt: {input_text}\\nExpected: {expected}\\nPredicted: {predicted}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540,
          "referenced_widgets": [
            "bee21d23d71640779c6eaa1f39c9fd6f",
            "51b2bdf2768c4534a6464cabee0bbc32",
            "7da99ccfface4309ad39e57057399f2e",
            "c237b0d2ca404de180a5292f0ec4493c",
            "8c5a61a1f5c84bc39470196172b04c32",
            "04dbbc0844b34ca89fe58195093548fa",
            "4d717cf033294a2bb1f49c70164d4951",
            "b9e16bc08f0e43afa9b9ef83b14913e2",
            "56685083ce644c84ab3665eb41e43e3b",
            "c9ecb66ee296462783c15c38cc0e4f56",
            "c96bf16b30da40c6a477a4d185f55d86"
          ]
        },
        "id": "fzvpcSjuDKqi",
        "outputId": "7ea7d27f-d0b2-4c88-bdc3-522ff5a3bc01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/603 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bee21d23d71640779c6eaa1f39c9fd6f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='302' max='151' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [151/151 01:51]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set evaluation: {'eval_loss': 0.25815361738204956, 'eval_runtime': 10.0459, 'eval_samples_per_second': 60.024, 'eval_steps_per_second': 15.031, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: What's the history of Rome?\n",
            "Expected: Sorry, I can only answer questions related to SQL and NoSQL databases.\n",
            "Predicted: What's the history of Rome?\n",
            "\n",
            "Prompt: Explain database denormalization.\n",
            "Expected: Denormalization intentionally duplicates data to improve read performance, common in analytics or NoSQL.\n",
            "Predicted: Explain database denormalization.\n",
            "\n",
            "Prompt: Write a Neo4j query to update a node.\n",
            "Expected: MATCH (n:Person {name: 'Alice'}) SET n.age = 31;\n",
            "Predicted: Write a Neo4j query to update a node.\n",
            "\n",
            "Prompt: Write a SQL query to join three tables\n",
            "Expected: SELECT * FROM table1 JOIN table2 ON table1.id = table2.id JOIN table3 ON table2.id = table3.id;\n",
            "Predicted: Write a SQL query to join three tables\n",
            "\n",
            "Prompt: Explain database transactions.\n",
            "Expected: Transactions ensure a series of operations complete atomically or roll back on failure.\n",
            "Predicted: Explain database transactions.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load fine-tuned model and tokenizer\n",
        "model_path = \"./fine_tuned_model\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Ensure model is on CUDA (if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Get custom prompt from user\n",
        "custom_prompt = input(\"Enter your prompt: \")\n",
        "\n",
        "# Tokenize prompt\n",
        "inputs = tokenizer(custom_prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "# Move inputs to the same device as model\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "# Generate response\n",
        "outputs = model.generate(**inputs, max_length=512, num_return_sequences=1)\n",
        "\n",
        "# Decode and print response\n",
        "predicted = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"Prompt: {custom_prompt}\\nPredicted: {predicted}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNwOCvPuJR9y",
        "outputId": "669f121d-a766-4ade-cd8d-c5600902e4ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your prompt: Write a SQL query to find employees by salary range.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: Write a SQL query to find employees by salary range.\n",
            "Predicted: Write a SQL query to find employees by salary range.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# Load fine-tuned model and tokenizer\n",
        "model_path = \"./fine_tuned_model\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Ensure model is on CUDA (if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Get custom prompt from user\n",
        "custom_prompt = input(\"Enter your prompt: \")\n",
        "\n",
        "# Format prompt for inference (mimic training format)\n",
        "formatted_prompt = f\"Prompt: {custom_prompt} Response:\"\n",
        "\n",
        "# Tokenize prompt\n",
        "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "# Move inputs to the same device as model\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "# Generate response with adjusted parameters\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_length=512,\n",
        "    num_return_sequences=1,\n",
        "    temperature=0.7,  # Encourage diverse outputs\n",
        "    top_p=0.9,        # Use nucleus sampling\n",
        "    do_sample=True,   # Enable sampling to avoid repetition\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Decode and print response\n",
        "predicted = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "# Extract response after \"Response:\" if present\n",
        "predicted = predicted.replace(formatted_prompt, \"\").strip()\n",
        "print(f\"Prompt: {custom_prompt}\\nPredicted: {predicted}\")\n",
        "\n",
        "# Quick dataset check: Look for similar prompts\n",
        "df = pd.read_json('dataset.jsonl', lines=True)\n",
        "similar_prompts = df[df['prompt'].str.contains(\"merge.*columns\", case=False, na=False)]\n",
        "print(f\"Found {len(similar_prompts)} similar prompts in dataset:\")\n",
        "print(similar_prompts[['prompt', 'completion']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGm0JR1LJ7Hn",
        "outputId": "051ef9b3-1dae-4ce1-c7fb-a578c8212d38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your prompt: Write a SQL query to find employees by salary range.\n",
            "Prompt: Write a SQL query to find employees by salary range.\n",
            "Predicted: \n",
            "Found 0 similar prompts in dataset:\n",
            "Empty DataFrame\n",
            "Columns: [prompt, completion]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_json('dataset.jsonl', lines=True)\n",
        "print(df.columns)  # Should show ['prompt', 'completion']"
      ],
      "metadata": {
        "id": "ldP2oprEKb45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48147cc1-cc76-4fa4-cacf-3f6fe1989ce0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['prompt', 'completion'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "non_strings = df[df['completion'].apply(lambda x: not isinstance(x, str))]\n",
        "print(\"Non-string completions:\", len(non_strings))\n",
        "print(non_strings.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIcFP5KQ49E9",
        "outputId": "62069544-2f26-4bf0-a37d-b246e8a7e616"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Non-string completions: 0\n",
            "Empty DataFrame\n",
            "Columns: [prompt, completion]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['completion'] = df['completion'].apply(lambda x: str(x) if pd.notnull(x) else \"Sorry, I can only answer questions related to SQL and NoSQL databases.\")\n",
        "df = df.dropna(subset=['prompt', 'completion'])  # Drop any remaining nulls"
      ],
      "metadata": {
        "id": "0jPdhaYw9nsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positives = df[df['completion'].str.startswith(\"Sorry\") == False]\n",
        "negatives = df[df['completion'].str.startswith(\"Sorry\")]\n",
        "print(\"Positives:\", len(positives), \"Negatives:\", len(negatives))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycPekHia9v1h",
        "outputId": "3340d6f2-cb22-4f09-feb5-0b686138cb72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positives: 8193 Negatives: 1815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "train_df.to_json('train.jsonl', orient='records', lines=True)\n",
        "val_df.to_json('val.jsonl', orient='records', lines=True)\n",
        "test_df.to_json('test.jsonl', orient='records', lines=True)\n",
        "print(\"Train:\", len(train_df), \"Val:\", len(val_df), \"Test:\", len(test_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SW7aZ3M09zcT",
        "outputId": "408d84d2-aa39-48b1-9f2c-32203697f7f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 8006 Val: 1001 Test: 1001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCR3G3U4FLq0",
        "outputId": "dbe04d87-195c-4dd3-abf6-50b1ff3eafe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.34.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.1.8)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2025.8.3)\n",
            "\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `Querybot` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `Querybot`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import load_dataset\n",
        "import os\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"json\", data_files={\"train\": \"train.jsonl\", \"validation\": \"val.jsonl\"})\n",
        "\n",
        "# Tokenize for chat format\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-7B-v0.1\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "def tokenize(examples):\n",
        "    messages = [{\"role\": \"user\", \"content\": p} for p in examples['prompt']]\n",
        "    formatted = [tokenizer.apply_chat_template([msg, {\"role\": \"assistant\", \"content\": c}], tokenize=False)\n",
        "                for msg, c in zip(messages, examples['completion'])]\n",
        "    return tokenizer(formatted, padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize, batched=True)\n",
        "\n",
        "# Load model with LoRA\n",
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mixtral-7B-v0.1\", device_map=\"auto\")\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Train\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./fine_tuned_model\",\n",
        "    num_train_epochs=4,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=5e-6,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_steps=500,\n",
        "    fp16=True,\n",
        "    report_to=\"none\"  # Or \"wandb\" for logging\n",
        ")\n",
        "trainer = Trainer(model=model, args=args, train_dataset=tokenized_datasets[\"train\"], eval_dataset=tokenized_datasets[\"validation\"])\n",
        "trainer.train()\n",
        "\n",
        "# Save\n",
        "model.save_pretrained(\"./fine_tuned_model\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "BgP5caHA-DaK",
        "outputId": "af97bdfe-ce6b-4abb-8681-492bc7954bf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "mistralai/Mixtral-7B-v0.1 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/mistralai/Mixtral-7B-v0.1/resolve/main/tokenizer_config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;31m# This is slightly better for only 1 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m             hf_hub_download(\n\u001b[0m\u001b[1;32m    480\u001b[0m                 \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m   1011\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1657\u001b[0m         \u001b[0;31m# Unauthorized => likely a token issue => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1659\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1546\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1547\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[0m\n\u001b[1;32m   1462\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1463\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1464\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    287\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_backoff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_on_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_on_status_codes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m429\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    458\u001b[0m             )\n\u001b[0;32m--> 459\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-68b5c8ef-7b22164c78c34366507ea493;754af522-2f57-47a5-85a3-5478f27d72f7)\n\nRepository Not Found for url: https://huggingface.co/mistralai/Mixtral-7B-v0.1/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-839327328.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Tokenize for chat format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mistralai/Mixtral-7B-v0.1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;31m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m         \u001b[0mtokenizer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m     \u001b[0mcommit_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m     resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    882\u001b[0m         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m         \u001b[0mTOKENIZER_CONFIG_FILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \"\"\"\n\u001b[0;32m--> 321\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;31m# We cannot recover from them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGatedRepoError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m             raise OSError(\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0;34mf\"{path_or_repo_id} is not a local folder and is not a valid model identifier \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;34m\"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to pass a token \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: mistralai/Mixtral-7B-v0.1 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aVh6pD25-Jcf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}